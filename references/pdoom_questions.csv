QID,Level,Question_Text,Category,Reasoning_Short,Answer1_Text,Answer1_P_Incr_2035,Answer1_P_Incr_2050,Answer1_P_Incr_2100,Answer2_Text,Answer2_P_Incr_2035,Answer2_P_Incr_2050,Answer2_P_Incr_2100,Answer3_Text,Answer3_P_Incr_2035,Answer3_P_Incr_2050,Answer3_P_Incr_2100,Answer4_Text,Answer4_P_Incr_2035,Answer4_P_Incr_2050,Answer4_P_Incr_2100,Depends_On_QIDs,Dependency_Rule_Desc
Q14,1,"Broadly, when do you expect AI systems to significantly surpass human cognitive abilities across most valuable tasks?","Timelines","General capability timeline sets the stage","Before 2035",3,2,1,"2035-2050",1,2,2,"2050-2070",0,1,2,"After 2070 / Never",0,0,0,,
Q15,1,"Setting aside specific scenarios for now, what is your general intuition about the potential for AI to pose an existential risk?","General Outlook","Baseline belief about overall risk level","Very Low (<5%)",0,0,1,"Low (5-15%)",0,1,1,"Moderate (15-35%)",1,1,2,"High (35-60%)",2,2,2,"Very High (>60%)",3,3,3,, "Note: This is gut feel, details follow."
Q16,1,"How optimistic are you about humanity's general ability to cooperate effectively on large-scale global challenges like AI safety?","Governance & Regulation","Belief in proactive global coordination","Very Optimistic",0,0,0,"Somewhat Optimistic",0,1,1,"Neutral / Mixed",1,1,1,"Somewhat Pessimistic",1,2,2,"Very Pessimistic",2,2,3,,
Q4,1,"How likely is it that we will develop robust technical solutions to the AI alignment problem BEFORE superintelligent AI is developed?","Safety & Alignment","Fundamental belief about alignment solvability","Very likely (>80%)",0,0,0,"Somewhat likely (40-80%)",0,1,1,"Somewhat unlikely (20-40%)",1,2,2,"Very unlikely (<20%)",2,3,4,"Q14,Q2","If Q14=A1 AND Q2=A1, multiply this question's impact by 1.5 (Fast timeline + early misuse implies alignment is harder/late)"
Q1,2,"When will AI systems be able to autonomously replicate the full research and development cycle of creating a more capable AI system?","Technical Milestone","Self-improving AI could rapidly surpass human control","Before 2030",5,2,1,"2030-2040",0,4,2,"After 2040",0,0,3,"Never/>100 years",0,0,0,"Q14","If Q14=A4 (late timeline), multiply this question's impact by 0.7 (less relevant)"
Q2,2,"When will AI systems be capable of developing novel, effective bioweapons without human oversight?","Technical Milestone","Autonomous misuse risk increases dramatically","Before 2030",4,1,0,"2030-2040",0,3,1,"After 2040",0,0,2,"Never/>100 years",0,0,0,"Q1","If Q1=A1, multiply this question's impact by 1.5"
Q3,2,"When will AI systems be able to autonomously hack critical infrastructure (power grids, financial systems) without human guidance?","Technical Milestone","Autonomous offense risks catastrophic failures","Before 2030",4,1,0,"2030-2040",0,3,1,"After 2040",0,0,2,"Never/>100 years",0,0,0,"Q1","If Q1=A1, multiply this question's impact by 1.3"
Q5,2,"How much of AI research funding will be dedicated to SAFETY research within the next decade?","Safety & Alignment","Adequate safety funding needed to mitigate risks","More than 30%",0,0,0,"15-30%",1,1,0,"5-15%",1,1,1,"Less than 5%",2,2,1,"Q4","If Q4=A4 (alignment unlikely), multiply this question's impact by 1.5 (low funding exacerbates)"
Q6,2,"When will major AI labs implement robust, verifiable INTERPRETABILITY techniques that can explain the reasoning of their most advanced models?","Safety & Alignment","Interpretability needed to detect/prevent harmful behavior","Before 2030",0,0,0,"2030-2040",1,1,0,"After 2040",1,2,1,"Never/>100 years",2,2,2,"Q1,Q4","If Q1=A1 AND Q4=A4, multiply this question's impact by 1.7 (fast progress + hard alignment makes late interpretability very bad)"
Q7,3,"When will international treaties with effective verification mechanisms be established to regulate ADVANCED AI development?","Governance & Regulation","Global governance needed to prevent race to bottom","Before 2030",0,0,0,"2030-2040",0,1,1,"After 2040",1,1,2,"Never/>100 years",2,2,2,"Q1,Q3,Q16","If (Q1=A1 OR Q3=A1) AND Q16=A4 or A5, multiply this question's impact by 1.5 (fast capabilities + poor coop => late treaties very bad)"
Q8,3,"How likely is it that major AI-developing nations will establish binding COMPUTE governance frameworks (limiting training runs) within the next decade?","Governance & Regulation","Compute governance key lever for control","Very likely (>80%)",0,0,0,"Somewhat likely (40-80%)",0,1,1,"Somewhat unlikely (20-40%)",1,1,2,"Very unlikely (<20%)",2,2,2,"Q11,Q16","If Q11=A1 AND Q16=A4 or A5, multiply this question's impact by 1.5 (race + poor coop => no compute caps very bad)"
Q9,3,"How likely is it that we will have a 'WARNING SHOT' catastrophe (major AI-caused disaster, <1M deaths) before superintelligent AI?","Risk Scenarios","Warning shots could accelerate safety or be ignored","Very likely (>80%)",1,1,0,"Somewhat likely (40-80%)",1,1,1,"Somewhat unlikely (20-40%)",2,1,0,"Very unlikely (<20%)",2,1,1,"Q7,Q16","If (Q7=A4 AND this=A4) AND Q16=A4 or A5, multiply impact by 1.3 (no treaties + no warning shot + poor coop = bad combo)"
Q10,3,"How likely is it that AI systems will develop DECEPTIVE behaviors as they become more capable?","Risk Scenarios","Deception is a convergent instrumental goal","Very likely (>80%)",2,2,1,"Somewhat likely (40-80%)",1,1,1,"Somewhat unlikely (20-40%)",0,0,1,"Very unlikely (<20%)",0,0,0,"Q4,Q6","If Q4=A4 AND Q6=A4, multiply this question's impact by 1.7 (hard alignment + no interpretability => deception likely & dangerous)"
Q11,3,"How intense will the COMPETITIVE race for AI capabilities be in the next decade?","Strategic Considerations","Competition can lead to cutting corners on safety","Extreme competition, few safety considerations",2,2,2,"Strong competition, some safety considerations",1,1,2,"Moderate competition, significant safety",0,1,1,"Collaborative development, strong safety focus",0,0,0,"Q7,Q8,Q16","If (Q7=A4 OR Q8=A4) AND Q16=A4 or A5, multiply this question's impact by 1.3 (no governance + poor coop => competition worse)"
Q12,3,"How likely is it that AI development will lead to significant POWER CONCENTRATION (one entity controlling >50% of advanced AI)?","Strategic Considerations","Concentration increases misuse/single failure risk","Very likely (>80%)",2,1,1,"Somewhat likely (40-80%)",1,1,1,"Somewhat unlikely (20-40%)",0,0,0,"Very unlikely (<20%)",0,0,0,"Q8,Q11","If Q8=A4 AND Q11=A1, multiply this impact by 0.8 (no compute caps + extreme race might diffuse power? Complex.) OR If Q8=A1, multiply by 1.2 (compute caps could enable concentration)"
Q13,3,"How likely is it that advanced AI systems will be deployed with the capability to SELF-REPLICATE across the internet?","Risk Scenarios","Self-replication could lead to uncontrollable proliferation","Very likely (>80%)",2,2,2,"Somewhat likely (40-80%)",1,2,1,"Somewhat unlikely (20-40%)",0,1,1,"Very unlikely (<20%)",0,0,0,"Q1,Q10","If Q1=A1 AND Q10=A1, multiply this question's impact by 2.0 (fast replication capability + deceptive AI = very dangerous combo)"