Expert_ID,Name,P_Doom_Estimate_Qualitative,P_Doom_Lower_Bound_Percent,P_Doom_Upper_Bound_Percent,Estimate_Confidence_Qualitative,Stated_Time_Horizon_Raw,Primary_Estimate_Horizon_Year,P_Doom_Estimate_By_2035_Percent,P_Doom_Estimate_By_2050_Percent,P_Doom_Estimate_By_2100_Percent,Reasoning_Categories,Reasoning_Summary,Source_URL,Estimate_Date,Interpretation_Notes
1,Eliezer Yudkowsky,>99%,99,100,Very High,"Implied Near-Term/Decades",2040,90,95,99,"Alignment Difficulty, Compute Overhang, Racing Dynamics","Claims current ML trajectory leads to doom due to alignment difficulty",https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities,2022-11,"Bounds from '>99%'. Timeline interp. based on strong pessimism & 'inevitable trajectory' argument implying near-mid term risk is extremely high."
2,Paul Christiano,~20%,15,25,Medium,"Implied Decades/Century",2070,5,10,20,"Alignment Difficulty, Governance Failure","Based on technical analysis of AI systems and potential solutions to alignment",https://www.lesswrong.com/posts/HduCjmXTBD4xYTegv/my-current-thoughts-on-ai-risk,2021-12,"Bounds from '~20%'. Midpoint 20% used notionally for 2070/2100. Timeline interp. based on technical focus suggesting risk scales with capability over decades."
3,Sam Altman,~33%,25,40,Medium,"Implied Century",2100,5,15,33,"Misuse, Control Issues","Concerned about misuse and control issues but optimistic about solutions",https://www.theatlantic.com/technology/archive/2023/07/sam-altman-openai-chatgpt-existential-risk/674701/,2023-07,"Bounds based on general estimate ~33%. Assigned main estimate to 2100, interpolated earlier based on 'optimistic on solutions' suggesting risk isn't immediate max."
4,Geoffrey Hinton,>50%,50,70,Medium,"30-50 years",2073,10,30,60,"Capability Growth, Control Issues","Changed views after leaving Google, concerned about AI surpassing human intelligence",https://www.reuters.com/technology/ai-pioneer-hinton-says-ai-could-pose-existential-threat-2023-05-05/,2023-05,"Bounds inferred from '>50%'. Used midpoint 60%. Horizon approx 2073 based on '30-50 years'. Interpolated timeline based on capability focus."
5,Stuart Russell,"Significant",30,70,High,"Implied Century",2100,5,20,50,"Misaligned Objectives, Control Issues","Argues current AI approaches are flawed and need rethinking",https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616,2019-10,"Interpreted 'Significant' as 30-70%, used midpoint 50% for 2100. Distributed risk over time based on 'fundamental flaw' argument scaling with capability."
6,Demis Hassabis,~10%,5,15,Medium,"Implied Century",2100,1,3,10,"Safety Solutions, Governance","Believes risks are manageable with proper safety measures",https://time.com/6288890/demis-hassabis-deepmind-ai/,2023-07,"Bounds from '~10%', used 10% midpoint for 2100. Low near-term reflecting belief in manageability, risk increasing later if solutions fail."
7,Elon Musk,10-20%,10,20,Medium,"5-10 years maybe, up to 20",2043,5,10,15,"Racing Dynamics, Governance Failure","Concerned about AI safety but continues to invest in AI development",https://pauseai.info/pdoom,2023-04,"Used 15% midpoint. Horizon interpretation based on varied statements focusing on near-mid term acceleration. Spread risk."
8,Yoshua Bengio,~10%,5,15,Medium,"Implied Century",2100,2,5,10,"Safety Research, Governance","Initially skeptical of risks, now advocates for AI safety research",https://www.technologyreview.com/2023/05/02/1072522/yoshua-bengio-worried-about-ai/,2023-05,"Bounds from '~10%', used 10% midpoint for 2100. Shift in view suggests increasing concern, but focus on research implies hope for mitigation."
9,Andrew Ng,<1%,0,1,High,"Unspecified/Long",2100,0,0,1,"Technical Limitations, Present Focus","Believes AI risks are overstated, focuses on practical applications",https://www.wired.com/story/andrew-ng-ai-pioneer-humans-work-with-ai/,2022-03,"Bounds from '<1%'. Assigned near zero risk across all timelines due to strong skepticism."
10,Dario Amodei,~15%,10,20,Medium,"Implied Century",2100,5,10,15,"Alignment Challenges, Safety Research","Concerned about alignment challenges but believes they're solvable",https://80000hours.org/podcast/episodes/dario-amodei-anthropic/,2022-05,"Bounds from '~15%', used 15% midpoint for 2100. Focus on solvable challenges suggests risk is not immediate but grows with capability."
11,Nick Bostrom,~30%,20,40,Medium,"Implied Century (Superintelligence)",2100,5,15,30,"Control Problem, Misaligned Objectives","Argues superintelligence poses existential risk if not properly aligned",https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834,2014-07,"Bounds from '~30%', used 30% midpoint for 2100. Risk tied to superintelligence, likely mid-to-long term based on book's themes."
12,Max Tegmark,~10%,5,15,Medium,"Implied Century",2100,2,5,10,"Governance, Long-term Risk","Advocates for beneficial AI research and governance",https://futureoflife.org/team/max-tegmark/,2021-09,"Bounds from '~10%', used 10% midpoint for 2100. Focus on governance and long-term suggests risk accrues over time."
13,Jaan Tallinn,~30%,20,40,Medium,"Implied Century",2100,5,15,30,"Self-improvement, Control Issues","Skype co-founder who funds AI safety research due to concerns",https://www.theguardian.com/technology/2016/feb/28/jaan-tallinn-artificial-intelligence-extinction-risk,2020-06,"Bounds from '~30%', used 30% midpoint for 2100. Concern about self-improvement implies risk scales significantly with capability."
14,Gary Marcus,~5%,2,10,Low,"Implied Century",2100,1,2,5,"Technical Limitations, Safety Solutions","Critical of current AI approaches but skeptical of imminent doom",https://garymarcus.substack.com/p/the-ai-safety-debate-heats-up,2023-04,"Bounds from '~5%', used 5% midpoint for 2100. Skepticism suggests low probability overall, increasing slightly over longer term."
15,Francesca Rossi,<5%,1,5,Medium,"Implied Century",2100,0,1,3,"Governance, Ethics","Focuses on AI ethics and believes proper governance can mitigate risks",https://research.ibm.com/people/francesca-rossi,2022-11,"Bounds from '<5%', used 3% midpoint for 2100. Belief in governance suggests low overall risk, weighted towards later periods if governance fails."
16,Yann LeCun,<1%,0,1,High,"Unspecified/Long",2100,0,0,1,"Technical Limitations, Skepticism","Believes AI risks are exaggerated, focuses on technical limitations",https://twitter.com/ylecun/status/1643976007729491970,2023-04,"Bounds from '<1%'. Assigned near zero risk across all timelines due to strong skepticism."
17,Melanie Mitchell,<5%,0,5,Medium,"Unspecified/Long",2100,0,1,3,"Technical Limitations, Skepticism","Skeptical of near-term AGI, emphasizes AI's current limitations",https://melaniemitchell.me/articles/,2022-08,"Bounds from '<5%', used 3% midpoint for 2100. Skepticism about near-term AGI pushes potential risk further out."
18,Timnit Gebru,<1%,0,1,High,"Focus on Present",2035,1,1,1,"Present Harms, Technical Limitations","Focuses on present-day AI harms rather than speculative risks",https://www.wired.com/story/timnit-gebru-artificial-intelligence-impact/,2022-12,"Bounds from '<1%'. Focus on present harms suggests low credence in future existential risk from AI itself. Assigned minimal risk across board."
19,Shane Legg,~50%,40,60,Medium,"Implied Century",2100,10,25,50,"Self-improvement, Control Issues","DeepMind co-founder who has expressed concerns about AI risks",https://www.lesswrong.com/posts/4QxzWpddECnkLo7sF/shane-legg-on-risks-from-ai,2011-05,"Bounds from '~50%', used 50% midpoint for 2100 (based on old but influential statements). Concern about self-improvement implies risk scales significantly."
20,Ben Goertzel,~10%,5,15,Low,"Implied Century",2100,2,5,10,"Design Solutions, Safety Engineering","AGI researcher who believes proper design can mitigate risks",https://goertzel.org/,2020-10,"Bounds from '~10%', used 10% midpoint for 2100. Belief in design solutions keeps estimate relatively low, risk accruing later."
21,Ilya Sutskever,~15%,10,20,Medium,"Implied Decades",2070,5,10,15,"Alignment Challenges, Capability Growth","OpenAI co-founder concerned about alignment challenges",https://www.nytimes.com/2023/05/01/technology/ai-threats-openai-scientists.html,2023-05,"Bounds from '~15%', used 15% midpoint assigned notionally to 2100 but potentially earlier. Concern about alignment + capabilities suggests mid-term risk."
22,Toby Ord,1/6 (~17%),10,20,High,"100",2100,2,5,17,"Existential Risk, Long-term Future","Estimates 1/6 chance of existential catastrophe from AI this century",https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911,2020-03,"Estimate is ~17%. Bounds added for range. Explicitly 'this century', assigned to 2100. Earlier values interp. low based on long-term focus."
23,Grady Booch,<5%,1,5,Medium,"Unspecified/Long",2100,0,1,3,"Engineering Solutions, Governance","IBM Fellow who believes AI risks are manageable with proper engineering",https://twitter.com/Grady_Booch,2021-07,"Bounds from '<5%', used 3% midpoint for 2100. Belief in engineering solutions suggests low risk weighted towards later periods."
24,Rodney Brooks,<1%,0,1,High,"Unspecified/Very Long",2100,0,0,1,"Technical Limitations, Timeline Skepticism","Robotics pioneer who believes AGI timelines are greatly exaggerated",https://rodneybrooks.com/my-dated-predictions/,2022-05,"Bounds from '<1%'. Assigned near zero risk across all timelines due to strong skepticism about AGI feasibility."
25,Anders Sandberg,~10%,5,15,Medium,"Implied Century",2100,2,5,10,"Existential Risk, Future of Humanity","Researcher focusing on existential risks and future of humanity",https://www.fhi.ox.ac.uk/team/anders-sandberg/,2021-11,"Bounds from '~10%', used 10% midpoint for 2100. Focus on X-risk analysis suggests considered view, risk accruing over century."
26,Bill Gates,~5%,2,10,Medium,"Implied Century",2100,1,2,5,"Governance, Misuse, Potential Benefits","Acknowledges risks but emphasizes potential benefits and manageability",https://www.gatesnotes.com/The-Age-of-AI-Has-Begun,2023-03,"Interpreted as low probability based on optimism about managing downsides. Used 5% midpoint for 2100."
27,Eric Schmidt,~15%,10,20,Medium,"Implied Decades/Century",2080,3,8,15,"National Security, Racing Dynamics","Former Google CEO concerned about US-China AI competition and military AI risks",https://www.foreignaffairs.com/united-states/eric-schmidt-artificial-intelligence-national-security,2023-05,"Bounds from ~15% inferred from strong concern. Horizon implies risk grows significantly mid-to-long term. Used 15% midpoint."
28,Mustafa Suleyman,~50%?,40,60,Low,"10-20 years",2040,20,40,50,"Containment Problem, Capability Growth","DeepMind co-founder, now Inflection AI CEO, author of 'The Coming Wave', highly concerned about containment",https://www.amazon.com/Coming-Wave-Technology-Power-Twenty-First/dp/0553447419,2023-09,"Estimate highly uncertain ('?'), bounds based on book's strong warnings. Midpoint 50% applied notionally. Short timeline implies rapid risk increase."
29,Ajeya Cotra,~15%,10,25,Medium,"By 2070",2070,5,10,15,"Alignment Difficulty, Timelines","Known for biological anchors report estimating AGI timelines, sees significant alignment risk",https://www.cold-takes.com/podcast-ajeya-cotra-on-the-most-important-century/,2022-07,"Bounds from '~15%' interp from overall work. Primary horizon linked to AGI timeline estimates. Used midpoint 15% split across time."
30,Roman Yampolskiy,>90% likely,80,99,High,"Implied Century",2100,30,60,90,"Unsolvability of Control, Capability Growth","Argues AI control problem is likely unsolvable before dangerous capabilities emerge",https://www.amazon.com/AI-Unexplainable-Unpredictable-Uncontrollable-Roman-Yampolskiy/dp/B0CGN66DBY,2023-08,"Bounds from strong pessimism '>90%'. Used 90% midpoint for 2100. Risk grows rapidly with capabilities."
31,Connor Leahy,~70%?,60,80,Low,"Implied Decades",2050,40,60,70,"Alignment Difficulty, Racing Dynamics","CEO of Conjecture, very vocal about near-term AI existential risk",https://www.youtube.com/watch?v=QGw5pW0y3SE,2023-06,"Estimate uncertain ('?'), based on strong pessimistic statements. Bounds reflect high concern. Short timeline implies high near/mid-term risk."
32,Dan Hendrycks,~25%,15,35,Medium,"Implied Century",2100,5,15,25,"Alignment Difficulty, Capability Benchmarking","Director of Center for AI Safety, works on evaluating AI risks",https://www.safe.ai/mission,2023-01,"Bounds inferred from focus on severe risks. Used 25% midpoint for 2100. Risk accrues with capability milestones."
33,Holden Karnofsky,~10-50%,10,50,Low,"Implied Century",2100,5,15,30,"Most Important Century, Transformative AI","Co-CEO of Open Philanthropy, sees AI as potentially transformative, with significant downside risk",https://www.cold-takes.com/most-important-century/,2021-07,"Wide bounds reflect range in his 'Most Important Century' series. Used 30% midpoint for 2100. Focus on transformative impact implies risk grows."
34,Katja Grace,~10% (median survey),5,20,Medium,"Survey (median ~2047 for HLAI)",2047,5,10,15,"Expert Surveys, Timelines","Leads AI Impacts project, aggregates expert opinion on AI timelines and risks",https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/,2023-01,"Represents median survey results, not personal view. Used ~10% risk based on survey Q about bad outcomes. Horizon linked to survey's HLAI median. Spread risk."
35,Daniel Dewey,~20%,10,30,Low,"Implied Century",2100,5,10,20,"Alignment Research, Governance","Former Open Phil AI safety lead, focused on technical alignment and strategy",https://forum.effectivealtruism.org/posts/xwBuXrAL48Q5yBSSP/author-daniel-dewey,2018-11,"Bounds inferred from focus on managing significant risks. Used 20% midpoint for 2100. Risk grows but hope for solutions."
36,Rohin Shah,~15%,10,25,Medium,"Implied Century",2100,5,10,15,"Alignment Newsletter, Technical Research","Leads Alignment Newsletter, tracks technical alignment progress and challenges",https://rohinshah.com/about/,2022-12,"Bounds inferred from deep engagement with alignment problems. Used 15% midpoint for 2100. Risk tied to alignment progress."
37,Richard Ngo,~25%?,15,40,Low,"Implied Decades/Century",2070,10,20,25,"Alignment Research, World Models","AI safety researcher at OpenAI, focused on scalable oversight and internal model alignment",https://alignedpriorities.org/researchers/richard-ngo,2023-03,"Estimate uncertain ('?'), bounds inferred from research focus on hard problems. Used 25% midpoint notionally. Risk grows with model sophistication."
38,Steve Omohundro,~30%?,20,50,Low,"Implied Century",2100,5,15,30,"Basic AI Drives, Control Issues","Early work on instrumental convergence (basic AI drives) suggests inherent risks",https://selfawaresystems.com/2008/02/15/the-basic-ai-drives/,2008-02,"Estimate uncertain ('?'), based on implications of his early influential work. Used 30% midpoint for 2100. Risk tied to agentic AI."
39,David Chalmers,~20%?,10,30,Low,"Implied Century (Consciousness/AGI)",2100,5,10,20,"Consciousness, Control Problem","Philosopher known for work on consciousness, has written about AI risks",https://consc.net/papers/singularity.pdf,2010-05,"Estimate uncertain ('?'), bounds inferred from philosophical engagement with superintelligence risks. Used 20% midpoint for 2100."
40,Peter Thiel,"Low?",1,10,Low,"Unspecified/Long",2100,0,1,5,"Skepticism, Geopolitics","Investor, generally skeptical of 'AGI doom', more focused on geopolitical/economic impacts",https://www.youtube.com/watch?v=AfJ7LQn8KjA,2022-04,"Estimate uncertain ('?'), bounds reflect skepticism but perhaps non-zero concern. Used 5% midpoint for 2100. Low risk overall."
41,Marc Andreessen,"Low?",0,5,Low,"Unspecified/Long",2100,0,0,1,"Techno-Optimism, Acceleration","Investor, strong techno-optimist, advocates for accelerating AI development",https://a16z.com/why-ai-will-save-the-world/,2023-06,"Estimate uncertain ('?'), bounds reflect extreme techno-optimism bordering on dismissal of risk. Used 1% midpoint for 2100."
42,Jürgen Schmidhuber,<1%,0,1,Medium,"Unspecified/Long",2100,0,0,1,"Technical Solutions, Optimism","AI researcher (LSTM pioneer), generally optimistic about AI and downplays existential risks",https://people.idsia.ch/~juergen/agi-threat-risk-paper.html,2015-05,"Bounds based on dismissive stance towards existential risk. Used 1% midpoint for 2100."
43,Ray Kurzweil,"Low (via merge)",1,10,Medium,"~2045 (Singularity)",2045,1,3,5,"Merging with AI, Transhumanism","Futurist predicting Singularity around 2045, sees merging with AI as solution/outcome",https://www.amazon.com/Singularity-Near-Humans-Transcend-Biology/dp/0143037889,2005-09,"Estimate based on belief in successful human-AI merge, avoiding 'us vs them'. Used 5% midpoint. Risk low if merge happens, timescale based on Singularity prediction."
44,Fei-Fei Li,"Low (Focus on Bias/Ethics)",0,5,Medium,"Implied Century",2100,0,1,3,"Ethics, Human-Centered AI","AI researcher focused on human-centered AI, ethics, and positive applications",https://hai.stanford.edu/people/fei-fei-li,2022-10,"Bounds reflect focus on present harms (bias, fairness) rather than existential risk. Used 3% midpoint for 2100. Low risk overall."
45,Oren Etzioni,"Low?",1,10,Low,"Unspecified/Long",2100,0,1,5,"Technical Limitations, Pragmatism","AI researcher and entrepreneur, skeptical of near-term AGI and associated doom scenarios",https://www.geekwire.com/2023/longtime-ai-expert-oren-etzioni-on-what-the-field-needs-now-and-whats-overhyped/,2023-03,"Estimate uncertain ('?'), bounds reflect pragmatic focus and skepticism. Used 5% midpoint for 2100."
46,Jeff Dean,"Low?",1,10,Low,"Implied Century",2100,1,2,5,"Engineering Focus, Scalability","Lead of Google AI, focus on large-scale systems and capabilities, less public focus on x-risk",https://ai.googleblog.com/,2023-01,"Estimate uncertain ('?'), bounds inferred from engineering focus and less public alarm. Used 5% midpoint for 2100."
47,Kai-Fu Lee,~10%,5,15,Medium,"Implied Century",2100,2,5,10,"Geopolitics, Economic Impact","Investor and former tech exec, focuses on US-China AI dynamics and economic shifts, acknowledges risks",https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X,2018-09,"Bounds reflect acknowledgement of risks alongside economic focus. Used 10% midpoint for 2100."
48,Stuart Armstrong,~20%?,10,30,Low,"Implied Century",2100,5,10,20,"Value Learning, AI Safety Research","Researcher at FHI focused on value learning and AI safety formalisms",https://www.fhi.ox.ac.uk/team/stuart-armstrong/,2019-05,"Estimate uncertain ('?'), bounds inferred from research focus on solving hard safety problems. Used 20% midpoint for 2100."
49,Victoria Krakovna,~20%?,10,30,Low,"Implied Century",2100,5,10,20,"Agent Foundations, Safety Research","AI safety researcher (formerly DeepMind, now Future of Life Institute), focused on agent foundations",https://vkrakovna.wordpress.com/,2022-08,"Estimate uncertain ('?'), bounds inferred from deep engagement with technical safety problems. Used 20% midpoint for 2100."
50,Luke Muehlhauser,"Declined to estimate?",5,50,Low,"Implied Century",2100,5,15,25,"AI Safety Research Landscape","Former lead of Open Phil's AI safety funding, wrote extensively on the topic",https://lukemuehlhauser.com/,"Declined explicit % but writings show deep concern. Wide bounds reflect uncertainty. Used 25% midpoint notionally for 2100."
51,Carl Shulman,~30%?,20,50,Low,"Implied Century",2100,5,15,30,"Forecasting, Cause Prioritization","Researcher associated with FHI/OpenPhil, focused on forecasting and cause prioritization incl. AI risk",https://www.openphilanthropy.org/research/author/carl-shulman/,2017-10,"Estimate uncertain ('?'), bounds inferred from deep analysis of AI risk factors. Used 30% midpoint for 2100."
52,Joseph Carlsmith,10-20% (conditional?),10,20,Medium,"Before 2070",2070,5,10,15,"Transformative AI Risks Report","Authored influential Open Phil report on power-seeking AI risks",https://www.openphilanthropy.org/research/power-seeking-ai/,2022-05,"Bounds based on report's analysis suggesting significant chance of misalignment risk before 2070. Used 15% midpoint."
53,Daniel Kahneman,"High Concern",20,50,Low,"Implied Century",2100,5,15,35,"Human Irrationality, AI Decision-Making","Nobel laureate psychologist, expressed concerns about AI surpassing human judgment",https://www.theguardian.com/technology/2021/may/02/daniel-kahneman-clearly-ai-is-going-to-win-how-people-can-adapt,2021-05,"Qualitative 'High Concern' interpreted into wide bounds 20-50%. Used 35% midpoint for 2100. Concern based on human limits."
54,Yuval Noah Harari,"Significant Concern",20,60,Low,"Implied Century",2100,5,20,40,"Hacking Humans, Societal Control","Historian, author of Sapiens, concerned about AI for surveillance, manipulation, and obsolescence of humans",https://www.ynharari.com/topic/ai/,2023-04,"Qualitative 'Significant Concern' interpreted into wide bounds 20-60%. Used 40% midpoint for 2100. Focus on societal disruption/control."
55,George Hotz,"Low",1,10,Low,"Unspecified",2100,0,1,5,"AGI is Far Off, Hype Skeptic","Programmer/Entrepreneur (comma.ai), generally dismissive of near-term AGI or x-risk",https://www.youtube.com/watch?v=TljY-tIoW_Q,2023-05,"Bounds reflect skepticism. Used 5% midpoint for 2100. Risk low due to timeline skepticism."
56,Erik Brynjolfsson,"Low (Focus on Economics)",1,10,Medium,"Implied Century",2100,1,3,5,"Economic Impacts, Productivity","Economist focused on AI's impact on productivity and labor, less on x-risk",https://www.gsb.stanford.edu/faculty-research/faculty/erik-brynjolfsson,2022-09,"Bounds reflect primary focus on economic effects, implying lower perceived x-risk. Used 5% midpoint for 2100."
57,Daron Acemoglu,"Moderate Concern (Inequality/Control)",5,20,Medium,"Implied Century",2100,2,5,10,"Automation Inequality, Misdirection of AI","Economist, concerned AI focus leads to excessive automation, inequality, and potential control issues",https://www.project-syndicate.org/columnist/daron-acemoglu,2023-01,"Bounds reflect concern about societal impacts, potentially escalating. Used 10% midpoint for 2100."
58,Ian Hogarth,"Significant Concern (State Race)",10,30,Medium,"Implied Decades/Century",2080,5,10,20,"AI Nationalism, Governance Failure","Investor, wrote 'AI Nationalism' essay, concerned about geopolitical race dynamics",https://www.ianhogarth.com/blog/ai-nationalism,2018-06,"Bounds reflect concern about racing dynamics exacerbating risks. Used 20% midpoint. Risk grows with geopolitical tension."
59,Audrey Tang,"Low (Focus on Democracy/Tools)",0,5,Medium,"Unspecified",2100,0,1,2,"AI for Democracy, Assistive Tech","Taiwan's Digital Minister, focus on using tech for democratic/social good",https://www.wired.com/story/audrey-tang-taiwan-digital-minister-interview/,2021-08,"Bounds reflect focus on beneficial uses and democratic control, implying low x-risk perception. Used 2% midpoint for 2100."
60,Marietje Schaake,"Moderate Concern (Governance)",5,20,Medium,"Implied Century",2100,2,5,10,"Geopolitics, Democratic Control","Policy analyst, former MEP, focus on tech governance and democratic oversight of AI",https://www.marietjeschaake.eu/,2023-02,"Bounds reflect concern about governance failures in managing powerful AI. Used 10% midpoint for 2100."
61,Sheila McIlraith,<5%?,0,5,Low,"Implied Century",2100,0,1,3,"AI Safety, Explainability","AI researcher focused on human-compatible AI, explainability, ethical AI",https://www.cs.toronto.edu/~sheila/,2020-11,"Estimate uncertain ('?'), bounds reflect focus on building safe/ethical AI, suggesting optimism about solvability. Used 3% midpoint for 2100."
62,Michael Littman,<5%?,0,5,Low,"Unspecified/Long",2100,0,1,3,"Reinforcement Learning, Skepticism","Computer scientist, expressed skepticism about anthropomorphizing AI and imminent superintelligence",https://twitter.com/mlittman/status/1654208946903130112,2023-05,"Estimate uncertain ('?'), bounds reflect skepticism about current AI leading to x-risk. Used 3% midpoint for 2100."
63,Pedro Domingos,<1%?,0,1,Low,"Unspecified/Long",2100,0,0,1,"Master Algorithm, Optimism","Author of 'The Master Algorithm', generally optimistic about AI/ML progress",https://homes.cs.washington.edu/~pedrod/,2019-08,"Estimate uncertain ('?'), bounds reflect strong optimism and focus on capability. Used 1% midpoint for 2100."
64,Steven Pinker,<1%,0,1,High,"Unspecified/Long",2100,0,0,1,"Rationality, Progress Skepticism","Psychologist/Author, known skeptic of AI x-risk ('sci-fi trope')",https://stevenpinker.com/pages/prediction-vs-prophecy-case-techno-optimism,2023-07,"Bounds reflect strong, stated skepticism. Used 1% midpoint for 2100."
65,Sam Harris,~10%?,5,20,Low,"Implied Century",2100,2,5,10,"Consciousness, Intelligence Risks","Author/Podcaster, discussed AI risk frequently, acknowledges significance",https://www.samharris.org/podcasts/making-sense-episodes/207-can-we-build-ai-without-losing-control-over-it,2020-06,"Estimate uncertain ('?'), bounds based on expressed concern but not extreme pessimism. Used 10% midpoint for 2100."
66,Lex Fridman,"Low-Moderate?",5,25,Low,"Implied Century",2100,3,8,15,"Love, Human-AI Interaction","Podcaster/AI Researcher, often discusses AI consciousness/love, balances optimism and risk awareness",https://lexfridman.com/podcast/,2023-01,"Estimate uncertain ('?'), wide bounds reflect balancing optimism with guest concerns. Used 15% midpoint for 2100."
67,Vitalik Buterin,~10%?,5,15,Low,"Implied Century",2100,2,5,10,"Coordination Problems, Longtermism","Ethereum founder, engaged with longtermism and AI risk ideas",https://vitalik.ca/general/2023/03/31/should_pause.html,2023-03,"Estimate uncertain ('?'), bounds based on thoughtful engagement but not alarmist stance. Used 10% midpoint for 2100."
68,Gillian Hadfield,"Moderate Concern (Governance)",10,30,Medium,"Implied Century",2100,3,10,20,"Legal/Regulatory Frameworks","Law/Economics professor, focuses on need for robust AI governance and regulation",https://www.vectorinstitute.ai/people/gillian-hadfield/,2022-11,"Bounds reflect strong focus on governance challenges implies significant underlying risk if unsolved. Used 20% midpoint for 2100."
69,Helen Toner,"Moderate Concern (Policy/Strategy)",10,30,Medium,"Implied Century",2100,3,10,20,"AI Policy, US-China Relations","Policy analyst (formerly CSET, OpenAI board), focuses on AI strategy and policy challenges",https://cset.georgetown.edu/expert/helen-toner/,2023-02,"Bounds reflect deep engagement with policy challenges indicating awareness of substantial risks. Used 20% midpoint for 2100."
70,Jack Clark,~15%?,10,25,Low,"Implied Century",2100,5,10,15,"AI Index, Safety/Policy","Co-chair AI Index, formerly OpenAI policy, co-founder Anthropic, tracks AI progress and policy",https://importai.substack.com/,2023-05,"Estimate uncertain ('?'), bounds reflect deep industry/policy insight and safety focus at Anthropic. Used 15% midpoint for 2100."
71,Anca Dragan,"Low-Moderate?",5,20,Low,"Implied Century",2100,2,5,10,"Human-Robot Interaction, Reward Learning","AI researcher focused on HRI and learning human preferences",https://people.eecs.berkeley.edu/~anca/,2021-10,"Estimate uncertain ('?'), bounds reflect focus on technical alignment solutions suggesting optimism but awareness of difficulty. Used 10% midpoint for 2100."
72,Pieter Abbeel,"Low?",1,10,Low,"Implied Century",2100,1,3,5,"Robotics, Reinforcement Learning","AI researcher/Entrepreneur (Covariant), focus on ML for robotics",https://people.eecs.berkeley.edu/~pabbeel/,2022-06,"Estimate uncertain ('?'), bounds reflect focus on current capabilities/applications, less public focus on x-risk. Used 5% midpoint for 2100."
73,Roger Penrose,"Very Low (Non-computational mind)",0,1,Medium,"Unspecified/Very Long",2100,0,0,1,"Consciousness, Gödel's Theorems","Physicist/Mathematician, argues human consciousness is non-computational, limiting AI",https://www.amazon.com/Emperors-New-Mind-Concerning-Computers/dp/0198784922,1989-01,"Bounds reflect belief that AI cannot replicate crucial aspects of human intelligence. Used 1% midpoint for 2100."
74,Joscha Bach,"Low (Simulated Reality/Philosophy)",1,10,Low,"Unspecified/Very Long",2100,1,2,5,"Nature of Intelligence, Consciousness","AI researcher/Philosopher, often discusses AI in context of broader theories of mind/reality",https://bach.ai/,2021-05,"Estimate uncertain ('?'), bounds reflect philosophical perspective often downplaying conventional x-risk narratives. Used 5% midpoint for 2100."
75,Kevin Kelly,"Very Low",0,2,Medium,"Unspecified/Long",2100,0,0,1,"Techno-Optimism, AI as Utility","Founding editor Wired, strong techno-optimist, sees AI as tool/partner",https://kk.org/thetechnium/what-technology-wants/,2010-10,"Bounds reflect strong techno-optimism. Used 1% midpoint for 2100."
76,John Carmack,"Low-Moderate?",5,20,Low,"Implied Decades/Century",2070,2,5,10,"AGI Path, Engineering Pragmatism","Programmer (id Software, Oculus), pursuing AGI, pragmatic approach",https://twitter.com/ID_AA_Carmack,2022-12,"Estimate uncertain ('?'), bounds reflect pragmatic focus, acknowledging risks but not extreme alarm. Used 10% midpoint notionally."
77,Scott Aaronson,"Moderate Concern",10,30,Medium,"Implied Century",2100,3,10,20,"Quantum Computing, Complexity Theory, Safety","Computer scientist, involved with OpenAI safety, aware of complexity/risks",https://scottaaronson.blog/,2023-05,"Bounds reflect engagement with safety and complexity issues. Used 20% midpoint for 2100."
78,Larry Page,"Unknown (Historically Optimistic?)",1,20,Very Low,"Unspecified",2100,2,5,10,"AGI Pursuit (Google Co-founder)","Google co-founder, historically driven AGI pursuit, current views unclear but presumed optimistic",N/A (Private Figure),N/A,"Highly speculative. Bounds reflect presumed optimism but allow for some risk awareness. Used 10% midpoint. Confidence Very Low."
79,Sergey Brin,"Unknown (Historically Optimistic?)",1,20,Very Low,"Unspecified",2100,2,5,10,"AGI Pursuit (Google Co-founder)","Google co-founder, historically driven AGI pursuit, current views unclear but presumed optimistic",N/A (Private Figure),N/A,"Highly speculative. Bounds reflect presumed optimism but allow for some risk awareness. Used 10% midpoint. Confidence Very Low."
80,Mark Zuckerberg,"Low (Focus on Metaverse/Connectivity?)",1,10,Low,"Implied Century",2100,1,3,5,"Metaverse, Social Tech","Meta CEO, focus on social tech/metaverse, less public emphasis on AGI x-risk",https://about.fb.com/news/,2023-01,"Estimate uncertain ('?'), bounds reflect focus area suggesting lower perceived AGI x-risk. Used 5% midpoint for 2100."
81,Reid Hoffman,"Low-Moderate?",5,25,Low,"Implied Century",2100,3,8,15,"Human Amplification, Co-evolution","Investor/Entrepreneur (LinkedIn), sees AI as amplifying humans, co-evolution",https://www.linkedin.com/in/reidhoffman/,2023-04,"Estimate uncertain ('?'), bounds reflect optimistic 'amplification' view but acknowledges risks. Used 15% midpoint for 2100."
82,Vinod Khosla,"Low?",0,5,Low,"Unspecified/Long",2100,0,0,1,"Techno-Optimism, Acceleration","Investor, strong techno-optimist, advocates rapid AI development",https://khoslaventures.com/blog/,2023-05,"Estimate uncertain ('?'), bounds reflect extreme techno-optimism. Used 1% midpoint for 2100."
86,Nando de Freitas,"Moderate Concern?",10,30,Low,"Implied Century",2100,5,10,20,"Scaling Laws, AGI Path (DeepMind)","AI Researcher (DeepMind), focus on scaling, believes 'game is over' for AGI via scale",https://twitter.com/NandoDF/status/1524784531785797633,2022-05,"Estimate uncertain ('?'), bounds reflect AGI confidence implying risks need solving. Used 20% midpoint for 2100."
87,Jeff Clune,"Moderate Concern?",10,30,Low,"Implied Century",2100,5,10,20,"Open-Endedness, AI Generating Algorithms","AI Researcher (formerly OpenAI/Uber, now UBC), focus on open-ended algorithms",https://arxiv.org/abs/1905.10985,2019-05,"Estimate uncertain ('?'), bounds reflect research on powerful AI methods suggesting risk awareness. Used 20% midpoint for 2100."
88,Kenneth Stanley,"Low (Focus on Novelty?)",1,10,Low,"Implied Century",2100,1,3,5,"Open-Endedness, Novelty Search","AI Researcher (formerly OpenAI/Uber), focus on novelty search, less on goal optimization",https://eplex.cs.ucf.edu/noveltysearch/userspage/,2020-01,"Estimate uncertain ('?'), bounds reflect focus on less 'convergent' AI paths, maybe lower risk perception. Used 5% midpoint for 2100."
89,Robert Miles,"Moderate Concern",10,30,Medium,"Implied Century",2100,3,10,20,"AI Safety Communicator","Science communicator (YouTube channel 'Robert Miles') specializing in AI safety concepts",https://www.youtube.com/c/RobertMilesAI,2023-01,"Bounds based on tenor of educational content explaining significant AI risks. Used 20% midpoint for 2100."
90,Neel Nanda,"Moderate Concern",10,30,Medium,"Implied Decades/Century",2070,5,10,20,"Mechanistic Interpretability Research","AI Safety Researcher focused on understanding internal workings of models",https://neelnanda.io/,2023-06,"Bounds reflect focus on critical safety area (interpretability) suggesting awareness of significant risk if unsolved. Used 20% midpoint."
91,Chris Olah,"Moderate Concern",10,30,Medium,"Implied Century",2100,5,10,20,"Interpretability, Safety Research (Anthropic)","AI Safety Researcher (Anthropic, formerly Google/OpenAI) focused on interpretability",https://distill.pub/authors/chris-olah/,2022-09,"Bounds reflect pioneering work in interpretability for safety, implies significant risk focus. Used 20% midpoint for 2100."
92,Rumman Chowdhury,"Low (Focus on Ethics/Bias)",0,5,Medium,"Implied Century",2100,0,1,3,"Responsible AI, Ethics, Bias","Responsible AI advocate, focuses on present harms, ethics, algorithmic bias",https://www.rummanchowdhury.com/,2023-02,"Bounds reflect primary focus on ethics/bias, lower stated concern for speculative x-risk. Used 3% midpoint for 2100."
93,Meredith Whittaker,"Low (Focus on Power/Labor)",0,5,Medium,"Implied Century",2100,0,1,3,"AI Power Concentration, Labor Impacts","President of Signal, critical of concentrated tech power and AI's impact on labor",https://www.meredithwhittaker.org/,2023-03,"Bounds reflect focus on social/political/economic harms rather than AI x-risk itself. Used 3% midpoint for 2100."
94,Eric Horvitz,"Low-Moderate?",5,20,Low,"Implied Century",2100,2,5,10,"Human-AI Collaboration, Safety (Microsoft)","Chief Scientific Officer Microsoft, long history in AI, focus on beneficial AI",https://www.microsoft.com/en-us/research/people/horvitz/,2022-11,"Estimate uncertain ('?'), bounds reflect focus on safety/benefits within large corp context. Used 10% midpoint for 2100."
95,Susan Schneider,"Moderate Concern?",10,30,Low,"Implied Century",2100,3,10,20,"AI Consciousness, Alien Minds","Philosopher/Cognitive Scientist, explores AI consciousness and its implications",https://schneiderwebsite.com/,2019-09,"Estimate uncertain ('?'), bounds reflect philosophical exploration of deep AI issues suggesting risk awareness. Used 20% midpoint for 2100."
96,Gerd Gigerenzer,"Low (Focus on Human Decision Making)",1,10,Low,"Unspecified",2100,0,1,5,"Risk Literacy, Simple Heuristics","Psychologist focused on human rationality and risk literacy, critical of AI hype",https://www.mpib-berlin.mpg.de/employees/gerd-gigerenzer,2021-08,"Estimate uncertain ('?'), bounds reflect focus on human fallibility vs AI perfection, skepticism of AI superiority claims. Used 5% midpoint for 2100."
97,Shoshana Zuboff,"Moderate Concern (Surveillance Capitalism)",5,20,Medium,"Implied Century",2100,2,5,10,"Surveillance Capitalism, Control","Author 'Age of Surveillance Capitalism', concerned about AI enabling corporate/state control",https://shoshanazuboff.com/,2019-01,"Bounds reflect concern focused on societal control/freedom risks enabled by AI, not necessarily AI agency x-risk. Used 10% midpoint for 2100."
98,James Lovelock (deceased),"Low?",0,5,Low,"Unspecified/Long",2100,0,1,2,"Gaia Hypothesis, AI as Successor?","Originator of Gaia Hypothesis, saw potential AI succession calmly",https://www.theguardian.com/environment/2019/jun/18/james-lovelock-the-biosphere-and-i-are-both-in-our-hundreds,2019-06,"Estimate uncertain ('?'), based on late-life views seeing AI as potential evolutionary step. Low 'doom' probability from human perspective. Used 2% midpoint."
99,Seth Baum,"Moderate Concern",10,25,Medium,"Implied Century",2100,3,8,15,"Global Catastrophic Risk Analyst","Executive Director of GCRI, analyzes various GCRs including AI",https://gcrinstitute.org/seth-baum/,2022-07,"Bounds reflect professional focus on GCRs, giving AI significant weight. Used 15% midpoint for 2100."
100,Robin Hanson,"Low (Focus on Brain Emulations 'Ems')",1,10,Medium,"Implied Century (Post-Em)",2100,1,2,5,"Brain Emulation, Future Economies","Economist/Futurist, detailed analysis of 'Age of Em', sees AI risk differently",https://www.overcomingbias.com/,2016-05,"Bounds reflect focus on 'Em' scenario, where risks differ from standard AGI x-risk narratives. Used 5% midpoint for 2100 (as proxy)."